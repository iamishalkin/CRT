{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "sys.path.append('../')\n",
    "from pytorch.common.datasets_parsers.av_parser import AVDBParser\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from pytorch.common.datasets_parsers.av_parser import AVDBParser\n",
    "from voice_feature_extraction import OpenSMILE\n",
    "from accuracy import Accuracy, Accuracy_regression\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(dataset_root, file_list, max_num_clips=0):\n",
    "    dataset_parser = AVDBParser(dataset_root, file_list,\n",
    "                                max_num_clips=max_num_clips)\n",
    "    data = dataset_parser.get_data()\n",
    "    print('clips count:', len(data))\n",
    "    print('frames count:', dataset_parser.get_dataset_size())\n",
    "    return data\n",
    "\n",
    "def calc_features(data, opensmile_root_dir, opensmile_config_path):\n",
    "    vfe = OpenSMILE(opensmile_root_dir, opensmile_config_path)\n",
    "\n",
    "    progresser = tqdm(iterable=range(0, len(data)),\n",
    "                      desc='calc audio features',\n",
    "                      total=len(data),\n",
    "                      unit='files')\n",
    "\n",
    "    feat, targets = [], []\n",
    "    for i in progresser:\n",
    "        print(len(data))\n",
    "        clip = data[i]\n",
    "        print(clip.wav_rel_path)\n",
    "\n",
    "        try:\n",
    "            voice_feat = vfe.process(clip.wav_rel_path)\n",
    "        except:\n",
    "            data.remove(clip)\n",
    "\n",
    "        feat.append(voice_feat)\n",
    "        targets.append(clip.labels)\n",
    "\n",
    "    print('feat count:', len(feat))\n",
    "    return np.asarray(feat, dtype=np.float32), np.asarray(targets, dtype=np.float32)\n",
    "\n",
    "def classification(X_train, X_test, y_train, y_test, accuracy_fn, pca_dim=100):\n",
    "    if pca_dim > 0:\n",
    "        pca_model = PCA(n_components=min(pca_dim, X_train.shape[1])).fit(X_train)\n",
    "        X_train = pca_model.transform(X_train)\n",
    "        X_test = pca_model.transform(X_test)\n",
    "\n",
    "    # shuffle\n",
    "    combined = list(zip(X_train, y_train))\n",
    "    random.shuffle(combined)\n",
    "    X_train[:], y_train[:] = zip(*combined)\n",
    "\n",
    "    # TODO: используйте классификаторы из sklearn\n",
    "\n",
    "    y_pred = []\n",
    "    accuracy_fn.by_clips(y_pred)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'exp_1'\n",
    "max_num_clips = 0 # загружайте только часть данных для отладки кода\n",
    "use_dump = False # используйте dump для быстрой загрузки рассчитанных фич из файла\n",
    "\n",
    "# dataset dir\n",
    "base_dir = 'D:/AVER'\n",
    "if 1:\n",
    "    train_dataset_root = 'C:/Users/ipmstud/Desktop/STCML/Ryerson/Video'\n",
    "    train_file_list = 'C:/Users/ipmstud/Desktop/STCML/Ryerson/train_data_with_landmarks.txt'\n",
    "    test_dataset_root = 'C:/Users/ipmstud/Desktop/STCML/Ryerson/Video'\n",
    "    test_file_list = 'C:/Users/ipmstud/Desktop/STCML/Ryerson/test_data_with_landmarks.txt'\n",
    "elif 1:\n",
    "    train_dataset_root = base_dir + '/OMGEmotionChallenge-master/omg_TrainVideos/preproc/frames'\n",
    "    train_file_list = base_dir + '/OMGEmotionChallenge-master/omg_TrainVideos/preproc/train_data_with_landmarks.txt'\n",
    "    test_dataset_root =base_dir + '/OMGEmotionChallenge-master/omg_ValidVideos/preproc/frames'\n",
    "    test_file_list = base_dir + '/OMGEmotionChallenge-master/omg_ValidVideos/preproc/valid_data_with_landmarks.txt'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "AVDB meta parsing:   0%|                                                                 | 0/33087 [00:00<?, ?images/s]\n",
      "\n",
      "\n",
      "\n",
      "AVDB meta parsing:   3%|█▌                                                   | 947/33087 [00:00<00:03, 9432.52images/s]\n",
      "\n",
      "\n",
      "\n",
      "AVDB meta parsing:   6%|███▏                                               | 2032/33087 [00:00<00:03, 10131.75images/s]\n",
      "\n",
      "\n",
      "\n",
      "AVDB meta parsing:   9%|████▊                                              | 3108/33087 [00:00<00:02, 10290.48images/s]\n",
      "\n",
      "\n",
      "\n",
      "AVDB meta parsing:  13%|██████▌                                            | 4297/33087 [00:00<00:02, 10323.99images/s]\n",
      "\n",
      "\n",
      "\n",
      "AVDB meta parsing:  16%|████████▏                                          | 5298/33087 [00:00<00:02, 10234.81images/s]\n",
      "\n",
      "\n",
      "\n",
      "AVDB meta parsing:  19%|█████████▉                                         | 6447/33087 [00:00<00:02, 10184.27images/s]\n",
      "\n",
      "\n",
      "\n",
      "AVDB meta parsing:  23%|███████████▌                                       | 7463/33087 [00:00<00:02, 10168.07images/s]\n",
      "\n",
      "\n",
      "\n",
      "AVDB meta parsing:  26%|█████████████▎                                     | 8597/33087 [00:00<00:02, 10214.43images/s]\n",
      "\n",
      "\n",
      "\n",
      "AVDB meta parsing:  29%|██████████████▉                                    | 9660/33087 [00:00<00:02, 10257.87images/s]\n",
      "\n",
      "\n",
      "\n",
      "AVDB meta parsing:  33%|████████████████▌                                 | 10952/33087 [00:01<00:02, 10358.90images/s]\n",
      "\n",
      "\n",
      "\n",
      "AVDB meta parsing:  36%|██████████████████                                | 11991/33087 [00:01<00:02, 10344.01images/s]\n",
      "\n",
      "\n",
      "\n",
      "AVDB meta parsing:  39%|███████████████████▋                              | 13022/33087 [00:01<00:01, 10341.32images/s]\n",
      "\n",
      "\n",
      "\n",
      "AVDB meta parsing:  43%|█████████████████████▎                            | 14099/33087 [00:01<00:01, 10297.51images/s]\n",
      "\n",
      "\n",
      "\n",
      "AVDB meta parsing:  46%|██████████████████████▊                           | 15125/33087 [00:01<00:01, 10285.38images/s]\n",
      "\n",
      "\n",
      "\n",
      "AVDB meta parsing:  49%|████████████████████████▍                         | 16150/33087 [00:01<00:01, 10283.14images/s]\n",
      "\n",
      "\n",
      "\n",
      "AVDB meta parsing:  52%|█████████████████████████▉                        | 17169/33087 [00:01<00:01, 10198.21images/s]\n",
      "\n",
      "\n",
      "\n",
      "AVDB meta parsing:  55%|███████████████████████████▍                      | 18153/33087 [00:01<00:01, 10063.64images/s]\n",
      "\n",
      "\n",
      "\n",
      "AVDB meta parsing:  59%|█████████████████████████████▎                    | 19356/33087 [00:01<00:01, 10103.84images/s]\n",
      "\n",
      "\n",
      "\n",
      "AVDB meta parsing:  61%|███████████████████████████████▎                   | 20331/33087 [00:02<00:01, 6882.23images/s]\n",
      "\n",
      "\n",
      "\n",
      "AVDB meta parsing:  65%|████████████████████████████████▉                  | 21343/33087 [00:03<00:01, 6992.83images/s]\n",
      "\n",
      "\n",
      "\n",
      "AVDB meta parsing:  68%|██████████████████████████████████▌                | 22390/33087 [00:03<00:01, 7100.16images/s]\n",
      "\n",
      "\n",
      "\n",
      "AVDB meta parsing:  71%|████████████████████████████████████▎              | 23526/33087 [00:03<00:01, 7218.87images/s]\n",
      "\n",
      "\n",
      "\n",
      "AVDB meta parsing:  74%|█████████████████████████████████████▊             | 24559/33087 [00:03<00:01, 7311.07images/s]\n",
      "\n",
      "\n",
      "\n",
      "AVDB meta parsing:  78%|███████████████████████████████████████▊           | 25812/33087 [00:03<00:00, 7428.53images/s]\n",
      "\n",
      "\n",
      "\n",
      "AVDB meta parsing:  81%|█████████████████████████████████████████▎         | 26822/33087 [00:03<00:00, 7501.56images/s]\n",
      "\n",
      "\n",
      "\n",
      "AVDB meta parsing:  84%|███████████████████████████████████████████        | 27944/33087 [00:03<00:00, 7583.07images/s]\n",
      "\n",
      "\n",
      "\n",
      "AVDB meta parsing:  88%|████████████████████████████████████████████▊      | 29078/33087 [00:03<00:00, 7670.72images/s]\n",
      "\n",
      "\n",
      "\n",
      "AVDB meta parsing:  92%|██████████████████████████████████████████████▋    | 30314/33087 [00:03<00:00, 7766.19images/s]\n",
      "\n",
      "\n",
      "\n",
      "AVDB meta parsing:  95%|████████████████████████████████████████████████▎  | 31366/33087 [00:04<00:00, 7833.01images/s]\n",
      "\n",
      "\n",
      "\n",
      "AVDB meta parsing:  98%|█████████████████████████████████████████████████▉ | 32415/33087 [00:04<00:00, 7886.23images/s]\n",
      "\n",
      "\n",
      "\n",
      "AVDB meta parsing: 100%|███████████████████████████████████████████████████| 33087/33087 [00:04<00:00, 7933.18images/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clips count: 1200\n",
      "frames count: 33087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "AVDB meta parsing:   0%|                                                                  | 0/6839 [00:00<?, ?images/s]\n",
      "\n",
      "\n",
      "\n",
      "AVDB meta parsing:  17%|████████▊                                           | 1156/6839 [00:00<00:00, 10001.54images/s]\n",
      "\n",
      "\n",
      "\n",
      "AVDB meta parsing:  33%|█████████████████                                   | 2236/6839 [00:00<00:00, 10364.97images/s]\n",
      "\n",
      "\n",
      "\n",
      "AVDB meta parsing:  41%|█████████████████████▊                               | 2810/6839 [00:00<00:00, 4712.67images/s]\n",
      "\n",
      "\n",
      "\n",
      "AVDB meta parsing:  56%|█████████████████████████████▉                       | 3864/6839 [00:00<00:00, 5557.83images/s]\n",
      "\n",
      "\n",
      "\n",
      "AVDB meta parsing:  72%|██████████████████████████████████████▎              | 4944/6839 [00:00<00:00, 6176.33images/s]\n",
      "\n",
      "\n",
      "\n",
      "AVDB meta parsing:  87%|██████████████████████████████████████████████       | 5940/6839 [00:00<00:00, 6594.62images/s]\n",
      "\n",
      "\n",
      "\n",
      "AVDB meta parsing: 100%|█████████████████████████████████████████████████████| 6839/6839 [00:00<00:00, 7025.23images/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clips count: 240\n",
      "frames count: 6839\n"
     ]
    }
   ],
   "source": [
    "# opensmile configuration\n",
    "opensmile_root_dir = 'C:/Users/ipmstud/Desktop/crt/opensmile-2.3.0'\n",
    "# TODO: поэкспериментируйте с различными конфигурационными файлами библиотеки OpenSmile\n",
    "opensmile_config_path = 'C:/Users/ipmstud/Desktop/crt/opensmile-2.3.0/config/avec2013.conf'\n",
    "\n",
    "# load dataset\n",
    "train_data = get_data(train_dataset_root, train_file_list, max_num_clips=max_num_clips)\n",
    "test_data = get_data(test_dataset_root, test_file_list, max_num_clips=max_num_clips)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1200"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "240"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openSMILE version  b'2.3.0rc1 (Rev. 1593:1650M)\\r'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "calc audio features:   0%|                                                                 | 0/1200 [00:00<?, ?files/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200\n",
      "None\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected str, bytes or os.PathLike object, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-9a1e3b7aa412>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# get features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain_feat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_targets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalc_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopensmile_root_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopensmile_config_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mtest_feat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_targets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalc_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopensmile_root_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopensmile_config_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-9e257c74191b>\u001b[0m in \u001b[0;36mcalc_features\u001b[1;34m(data, opensmile_root_dir, opensmile_config_path)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;31m#         try:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m         \u001b[0mvoice_feat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvfe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclip\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwav_rel_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;31m#         except:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclip\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\crt\\audio_feature_classification\\voice_feature_extraction.py\u001b[0m in \u001b[0;36mprocess\u001b[1;34m(self, wav_path)\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[1;31m## example: SMILExtract -I output.wav -C ./openSMILE-2.1.0/config/gemaps/GeMAPSv01a.conf --csvoutput features.csv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[1;31m##----------------------------------------------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m         \u001b[0mfeatures_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwav_path\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'/temp.csv'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m         command = \"{opensmile_dir}/bin/Win32/SMILExtract_Release -I {input_file} -C {conf_file} --csvoutput {output_file}\".format(\n\u001b[0;32m     57\u001b[0m                         \u001b[0mopensmile_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopensmile_dir\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\moc\\IPM\\Anaconda3\\lib\\ntpath.py\u001b[0m in \u001b[0;36mdirname\u001b[1;34m(p)\u001b[0m\n\u001b[0;32m    241\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdirname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m     \u001b[1;34m\"\"\"Returns the directory component of a pathname\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 243\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    244\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    245\u001b[0m \u001b[1;31m# Is a path a symbolic link?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\moc\\IPM\\Anaconda3\\lib\\ntpath.py\u001b[0m in \u001b[0;36msplit\u001b[1;34m(p)\u001b[0m\n\u001b[0;32m    203\u001b[0m     \u001b[0mReturn\u001b[0m \u001b[0mtuple\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtail\u001b[0m\u001b[1;33m)\u001b[0m \u001b[0mwhere\u001b[0m \u001b[0mtail\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0meverything\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfinal\u001b[0m \u001b[0mslash\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m     Either part may be empty.\"\"\"\n\u001b[1;32m--> 205\u001b[1;33m     \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    206\u001b[0m     \u001b[0mseps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_bothseps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m     \u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msplitdrive\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not NoneType"
     ]
    }
   ],
   "source": [
    "# get features\n",
    "train_feat, train_targets = calc_features(train_data, opensmile_root_dir, opensmile_config_path)\n",
    "test_feat, test_targets = calc_features(test_data, opensmile_root_dir, opensmile_config_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "accuracy_fn = Accuracy(test_data, experiment_name=experiment_name)\n",
    "\n",
    "with open(experiment_name + '.pickle', 'wb') as f:\n",
    "    pickle.dump([train_feat, train_targets, test_feat, test_targets, accuracy_fn], f, protocol=2)\n",
    "else:\n",
    "with open(experiment_name + '.pickle', 'rb') as f:\n",
    "    train_feat, train_targets, test_feat, test_targets, accuracy_fn = pickle.load(f)\n",
    "\n",
    "# run classifiers\n",
    "classification(train_feat, test_feat, train_targets, test_targets, accuracy_fn=accuracy_fn, pca_dim=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
